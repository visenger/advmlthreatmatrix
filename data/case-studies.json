[
    {
        "id": "AML.CS0000",
        "name": "Evasion of Deep Learning detector for malware C&C traffic",
        "object-type": "case-study",
        "summary": "Palo Alto Networks Security AI research team tested a deep learning model for malware command and control (C&C) traffic detection in HTTP traffic.\nBased on the publicly available paper by Le et al.  [1], we built a model that was trained on a similar dataset as our production model and had performance similar to it.\nThen we crafted adversarial samples and queried the model and adjusted the adversarial sample accordingly till the model was evaded.\n",
        "incident-date": "2020-01-01",
        "procedure": [
            {
                "tactic": "TA0043",
                "technique": "AML.T0000.001",
                "description": "We identified a machine learning based approach to malicious URL detection as a representative approach and potential target from the paper \"URLNet: Learning a URL representation with deep learning for malicious URL detection\" [1], which was found on arXiv (a pre-print repository).\n"
            },
            {
                "tactic": "TA0042",
                "technique": "AML.T0002.000",
                "description": "We acquired a similar dataset to the target production model.\n"
            },
            {
                "tactic": "TA0042",
                "technique": "AML.T0005",
                "description": "We built a model that was trained on a similar dataset as the production model.\nWe trained the model on ~ 33 million benign and ~ 27 million malicious HTTP packet headers.\nEvaluation showed a true positive rate of ~ 99% and false positive rate of ~0.01%, on average.\nTesting the model with a HTTP packet header from known malware command and control traffic samples was detected as malicious with high confidence (> 99%).\n"
            },
            {
                "tactic": "AML.TA0001",
                "technique": "AML.T0043.003",
                "description": "We crafted evasion samples by removing fields from packet header which are typically not used for C&C communication (e.g. cache-control, connection, etc.)\n"
            },
            {
                "tactic": "AML.TA0001",
                "technique": "AML.T0042",
                "description": "We queried the model with our adversarial examples and adjusted them until the model was evaded.\n"
            },
            {
                "tactic": "TA0040",
                "technique": "AML.T0015",
                "description": "With the crafted samples we performed online evasion of the ML-based spyware detection model.\nThe crafted packets were identified as benign with >80% confidence.\nThis evaluation demonstrates that adversaries are able to bypass advanced ML detection techniques, by crafting samples that are misclassified by an ML model.\n"
            }
        ],
        "reported-by": [
            "Palo Alto Networks (Network Security AI Research Team)"
        ],
        "references": [
            "Le, Hung, et al. \"URLNet: Learning a URL representation with deep learning for malicious URL detection.\" arXiv preprint arXiv:1802.03162 (2018)."
        ]
    },
    {
        "id": "AML.CS0001",
        "name": "Botnet Domain Generation Algorithm (DGA) Detection Evasion",
        "object-type": "case-study",
        "summary": "The Palo Alto Networks Security AI research team was able to bypass a Convolutional Neural Network (CNN)-based botnet Domain Generation Algorithm (DGA) detection [1] by domain name mutations.\nIt is a generic domain mutation technique which can evade most ML-based DGA detection modules.\nThe generic mutation technique can also be used to test the effectiveness and robustness of all DGA detection methods developed by security companies in the industry before it is deployed to the production environment.\n",
        "incident-date": "2020-01-01",
        "procedure": [
            {
                "tactic": "TA0043",
                "technique": "AML.T0000",
                "description": "DGA detection is a widely used technique to detect botnets in academia and industry.\nThe searched for research papers related to DGA detection.\n"
            },
            {
                "tactic": "TA0042",
                "technique": "AML.T0002",
                "description": "The researchers acquired a publicly available CNN-based DGA detection model [1] and tested against a well-known DGA generated domain name data sets, which includes ~50 million domain names from 64 botnet DGA families.\nThe CNN-based DGA detection model shows more than 70% detection accuracy on 16 (~25%) botnet DGA families.\n"
            },
            {
                "tactic": "TA0042",
                "technique": "AML.T0017",
                "description": "The researchers developed a generic mutation technique that requires a minimal number of iterations.\n"
            },
            {
                "tactic": "AML.TA0001",
                "technique": "AML.T0043.001",
                "description": "The researchers used the mutation technique to generate evasive domain names.\n"
            },
            {
                "tactic": "AML.TA0001",
                "technique": "AML.T0042",
                "description": "Experiment results show that, after only one string is inserted once to the DGA generated domain names, the detection rate of all 16 botnet DGA families can drop to less than 25% detection accuracy.\n"
            },
            {
                "tactic": "TA0005",
                "technique": "AML.T0015",
                "description": "The DGA generated domain names mutated with this technique successfully evade the target DGA Detection model, allowing an adversary to continue communication with their [Command and Control]() servers.\n"
            }
        ],
        "reported-by": [
            "Palo Alto Networks (Network Security AI Research Team)"
        ],
        "references": [
            "[1] Yu, Bin, Jie Pan, Jiaming Hu, Anderson Nascimento, and Martine De Cock. \"Character level based detection of DGA domain names.\" In 2018 International Joint Conference on Neural Networks (IJCNN), pp. 1-8. IEEE, 2018. Source code is available from Github: https://github.com/matthoffman/degas"
        ]
    },
    {
        "id": "AML.CS0002",
        "name": "VirusTotal Poisoning",
        "object-type": "case-study",
        "summary": "An increase in reports of a certain ransomware family that was out of the ordinary was noticed.\nIn investigating the case, it was observed that many samples of that particular ransomware family were submitted through a popular Virus-Sharing platform within a short amount of time.\nFurther investigation revealed that based on string similarity, the samples were all equivalent, and based on code similarity they were between 98 and 74 percent similar.\nInterestingly enough, the compile time was the same for all the samples.\nAfter more digging, the discovery was made that someone used 'metame' a metamorphic code manipulating tool to manipulate the original file towards mutant variants.\nThe variants wouldn't always be executable but still classified as the same ransomware family.\n",
        "incident-date": "2020-01-01",
        "procedure": [
            {
                "tactic": "TA0042",
                "technique": "AML.T0016",
                "description": "The actor obtained [metame](https://github.com/a0rtega/metame), a simple metamorphic code engine for arbitrary executables.\n"
            },
            {
                "tactic": "AML.TA0001",
                "technique": "AML.T0043",
                "description": "The actor used a malware sample from a prevalent ransomware family as a start to create 'mutant' variants.\n"
            },
            {
                "tactic": "TA0001",
                "technique": "AML.T0010.002",
                "description": "The actor uploaded \"mutant\" samples to the platform.\n"
            },
            {
                "tactic": "TA0003",
                "technique": "AML.T0020",
                "description": "Several vendors started to classify the files as the ransomware family even though most of them won't run.\nThe \"mutant\" samples poisoned the dataset the ML model(s) use to identify and classify this ransomware family.\n"
            }
        ],
        "reported-by": [
            "Christiaan Beek (@ChristiaanBeek) - McAfee Advanced Threat Research"
        ]
    },
    {
        "id": "AML.CS0003",
        "name": "Bypassing Cylance's AI Malware Detection",
        "object-type": "case-study",
        "summary": "Researchers at Skylight were able to create a universal bypass string that\nwhen appended to a malicious file evades detection by Cylance's AI Malware detector.\n",
        "incident-date": "2019-09-07",
        "procedure": [
            {
                "tactic": "TA0043",
                "technique": "T1594",
                "description": "The researchers read publicly available information about Cylance's AI Malware detector.\n"
            },
            {
                "tactic": "AML.TA0000",
                "technique": "AML.T0047",
                "description": "The researchers used Cylance's AI Malware detector and enabled verbose logging to understand the inner workings of the ML model, particularly around reputation scoring.\n"
            },
            {
                "tactic": "TA0042",
                "technique": "AML.T0017",
                "description": "The researchers used the reputation scoring information to reverse engineer which attributes provided what level of positive or negative reputation.\nAlong the way, they discovered a secondary model which was an override for the first model.\nPositive assessments from the second model overrode the decision of the core ML model.\n"
            },
            {
                "tactic": "AML.TA0001",
                "technique": "AML.T0043.003",
                "description": "Using this knowledge, the researchers fused attributes of known good files with malware to manually create adversarial malware.\n"
            },
            {
                "tactic": "TA0040",
                "technique": "AML.T0015",
                "description": "Due to the secondary model overriding the primary, the researchers were effectively able to bypass the ML model.\n"
            }
        ],
        "reported-by": [
            "Research and work by Adi Ashkenazy, Shahar Zini, and SkyLight Cyber team.Notified to us by Ken Luu (@devianz_)"
        ],
        "references": [
            "https://skylightcyber.com/2019/07/18/cylance-i-kill-you/"
        ]
    },
    {
        "id": "AML.CS0004",
        "name": "Camera Hijack Attack on Facial Recognition System",
        "object-type": "case-study",
        "summary": "This type of attack can break through the traditional live detection model\nand cause the misuse of face recognition.\n",
        "incident-date": "2020-01-01",
        "procedure": [
            {
                "tactic": "TA0042",
                "technique": "T1583",
                "description": "The attackers bought customized low-end mobile phones.\n"
            },
            {
                "tactic": "TA0042",
                "technique": "T1588.002",
                "description": "The attackers obtained customized android ROMs and a virtual camera application.\n"
            },
            {
                "tactic": "TA0042",
                "technique": "AML.T0016",
                "description": "The attackers obtained software that turns static photos into videos, adding realistic effects such as blinking eyes.\n"
            },
            {
                "tactic": "TA0009",
                "technique": "T1213",
                "description": "The attackers collected user identity information and face photos.\n"
            },
            {
                "tactic": "TA0042",
                "technique": "T1585",
                "description": "The attackers registered accounts with the victims' identity information.\n"
            },
            {
                "tactic": "AML.TA0000",
                "technique": "AML.T0047",
                "description": "The attackers used the virtual camera app to present the generated video to the ML-based facial recongition product used for user verification.\n"
            },
            {
                "tactic": "TA0040",
                "technique": "AML.T0015",
                "description": "The attackers successfully evaded the face recognition system and impersonated the victim.\n"
            }
        ],
        "reported-by": [
            "Henry Xuef, Ant Group AISEC Team"
        ],
        "references": null
    },
    {
        "id": "AML.CS0005",
        "name": "Attack on Machine Translation Service - Google Translate, Bing Translator, and Systran Translate",
        "object-type": "case-study",
        "summary": "Machine translation services (such as Google Translate, Bing Translator, and Systran Translate) provide public-facing UIs and APIs.\nA research group at UC Berkeley utilized these public endpoints to create an replicated model with near-production, state-of-the-art translation quality.\nBeyond demonstrating that IP can be stolen from a black-box system, they used the replicated model to successfully transfer adversarial examples to the real production services.\nThese adversarial inputs successfully cause targeted word flips, vulgar outputs, and dropped sentences on Google Translate and Systran Translate websites.\n",
        "incident-date": "2020-04-30",
        "procedure": [
            {
                "tactic": "TA0043",
                "technique": "AML.T0000",
                "description": "The researchers used published research papers to identify the datasets and model architectures used by the target translaation services.\n"
            },
            {
                "tactic": "TA0042",
                "technique": "AML.T0002.000",
                "description": "The researchers gathered similar datasets that the target translation services used.\n"
            },
            {
                "tactic": "TA0042",
                "technique": "AML.T0002.001",
                "description": "The researchers gathered similar model architectures that the target translation services used.\n"
            },
            {
                "tactic": "AML.TA0000",
                "technique": "AML.T0040",
                "description": "They abuse a public facing application to query the model and produce machine translated sentence pairs as training data.\n"
            },
            {
                "tactic": "AML.TA0001",
                "technique": "AML.T0006",
                "description": "Using these translated sentence pairs, the researchers trained a model that replicates the behavior of the target model.\n"
            },
            {
                "tactic": "TA0040",
                "technique": "AML.T0045",
                "description": "By replicating the model with high fidelity, the researchers demonstrated that an adversary could steal a model and violate the victim's intellectual property rights.\n"
            },
            {
                "tactic": "AML.TA0001",
                "technique": "AML.T0043.002",
                "description": "The replicated models were used to generate adversarial examples that successfully transferred to the black-box translation services.\n"
            },
            {
                "tactic": "TA0040",
                "technique": "AML.T0015",
                "description": "The adversarial examples were used to evade the machine translation services.\n"
            }
        ],
        "reported-by": [
            "Work by Eric Wallace, Mitchell Stern, Dawn Song and reported by Kenny Song (@helloksong)"
        ],
        "references": [
            "https://arxiv.org/abs/2004.15015",
            "https://www.ericswallace.com/imitation"
        ]
    },
    {
        "id": "AML.CS0006",
        "name": "ClearviewAI Misconfiguration",
        "object-type": "case-study",
        "summary": "Clearview AI's source code repository, though password protected, was misconfigured to allow an arbitrary user to register an account.\nThis allowed an external researcher to gain access to a private code repository that contained Clearview AI production credentials, keys to cloud storage buckets containing 70K video samples, and copies of its applications and Slack tokens.\nWith access to training data, a bad-actor has the ability to cause an arbitrary misclassification in the deployed model.\nThese kinds of attacks illustrate that any attempt to secure ML system should be on top of \"traditional\" good cybersecurity hygiene such as locking down the system with least privileges, multi-factor authentication and monitoring and auditing.\n",
        "incident-date": "2020-04-16",
        "procedure": [
            {
                "tactic": "TA0001",
                "technique": "T1078",
                "description": "In this scenario, a security researcher gained initial access to via a valid account that was created through a misconfiguration.\n"
            }
        ],
        "reported-by": [
            "Mossab Hussein (@mossab_hussein)"
        ],
        "references": [
            "https://techcrunch.com/2020/04/16/clearview-source-code-lapse/amp/",
            "https://gizmodo.com/we-found-clearview-ais-shady-face-recognition-app-1841961772"
        ]
    },
    {
        "id": "AML.CS0007",
        "name": "GPT-2 Model Replication",
        "object-type": "case-study",
        "summary": "OpenAI built GPT-2, a powerful natural language model and adopted a staged-release process to incrementally release 1.5 Billion parameter model. Before the 1.5B parameter model could be released by OpenAI eventually, two ML researchers replicated the model and released it to the public.\n",
        "incident-date": "2019-08-22",
        "procedure": [
            {
                "tactic": "TA0043",
                "technique": "AML.T0000",
                "description": "Using the public documentation about GPT-2, ML researchers gathered information about the dataset, model architecture, and training hyper-parameters.\n"
            },
            {
                "tactic": "TA0042",
                "technique": "AML.T0002.001",
                "description": "The researchers obtained a reference implementation of a similar publicly available model called Grover.\n"
            },
            {
                "tactic": "TA0042",
                "technique": "AML.T0002.000",
                "description": "The researchers were able to manually recreate the dataset used in the original GPT-2 paper using the gathered documentation.\n"
            },
            {
                "tactic": "TA0042",
                "technique": "AML.T0008",
                "description": "The researchers were able to use TensorFlow Research Cloud via their academic credentials.\n"
            },
            {
                "tactic": "AML.TA0001",
                "technique": "AML.T0005",
                "description": "The researchers modified Grover's objective function to reflect GPT-2's objective function and then trained on the dataset they curated.\nThey used Grover's initial hyperparameters for training.\nThis resulted in their replicated model.\n"
            }
        ],
        "reported-by": [
            "Vanya Cohen (@VanyaCohen)",
            "Aaron Gokaslan (@SkyLi0n)",
            "Ellie Pavlick",
            "Stefanie Tellex"
        ],
        "references": [
            "https://www.wired.com/story/dangerous-ai-open-source/",
            "https://blog.usejournal.com/opengpt-2-we-replicated-gpt-2-because-you-can-too-45e34e6d36dc"
        ]
    },
    {
        "id": "AML.CS0008",
        "name": "ProofPoint Evasion",
        "object-type": "case-study",
        "summary": "CVE-2019-20634 describes how ML researchers evaded ProofPoint's email protection system by first building a copy-cat email protection ML model, and using the insights to evade the live system.\n",
        "incident-date": "2019-09-09",
        "procedure": [
            {
                "tactic": "TA0042",
                "technique": "AML.T0002",
                "description": "The researchers first gathered the scores from the Proofpoint's ML system used in email headers by sending a large number of emails through the system and scraping the model scores exposed in the logs.\n"
            },
            {
                "tactic": "TA0042",
                "technique": "AML.T0002.000",
                "description": "The researchers converted the collected scores into a dataset.\n"
            },
            {
                "tactic": "AML.TA0001",
                "technique": "AML.T0005",
                "description": "Using these scores, the researchers replicated the ML mode by building a \"shadow\" aka copy-cat ML model.\n"
            },
            {
                "tactic": "AML.TA0001",
                "technique": "AML.T0043.000",
                "description": "Next, the ML researchers algorithmically found samples that this \"offline\" copy cat model.\n"
            },
            {
                "tactic": "AML.TA0001",
                "technique": "AML.T0043.002",
                "description": "Finally, these insights from the offline model allowed the researchers to create malicious emails that received preferable scores from the real ProofPoint email protection system, hence bypassing it.\n"
            }
        ],
        "reported-by": [
            "Will Pearce (@moo_hax)",
            "Nick Landers (@monoxgas)"
        ],
        "references": [
            "https://nvd.nist.gov/vuln/detail/CVE-2019-20634",
            "https://github.com/moohax/Talks/blob/master/slides/DerbyCon19.pdf",
            "https://github.com/moohax/Proof-Pudding"
        ]
    },
    {
        "id": "AML.CS0009",
        "name": "Tay Poisoning",
        "object-type": "case-study",
        "summary": "Microsoft created Tay, a twitter chatbot for 18 to 24 year-olds in the U.S. for entertainment purposes. Within 24 hours of its deployment, Tay had to be decommissioned because it tweeted reprehensible words.\n",
        "incident-date": "2016-03-23",
        "procedure": [
            {
                "tactic": "AML.TA0000",
                "technique": "AML.T0040",
                "description": "Adversaries were able to interact with Tay via a few different publicly available methods.\n"
            },
            {
                "tactic": "TA0001",
                "technique": "AML.T0010.002",
                "description": "Tay bot used the interactions with its twitter users as training data to improve its conversations.\nAdversaries were able to coordinate with the intent of defacing Tay bot by exploiting this feedback loop.\n"
            },
            {
                "tactic": "TA0003",
                "technique": "AML.T0020",
                "description": "By repeatedly interacting with Tay using racist and offensive language, they were able to bias Tay's dataset towards that language as well.\n"
            },
            {
                "tactic": "TA0040",
                "technique": "AML.T0031",
                "description": "As a result of this coordinated attack, Tay's conversation algorithms began to learn to generate reprehensible material.\nThis quickly lead to its decommissioning.\n"
            }
        ],
        "reported-by": [
            "Microsoft"
        ],
        "references": [
            "https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/",
            "https://spectrum.ieee.org/tech-talk/artificial-intelligence/machine-learning/in-2016-microsofts-racist-chatbot-revealed-the-dangers-of-online-conversation"
        ]
    },
    {
        "id": "AML.CS0010",
        "name": "Microsoft - Azure Service",
        "object-type": "case-study",
        "summary": "The Azure Red Team and Azure Trustworthy ML team performed a red team exercise on an internal Azure service with the intention of disrupting its service. This operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API -- all interleaved with adversarial ML specific steps such as offline and online evasion examples.",
        "incident-date": "2020-01-01",
        "procedure": [
            {
                "tactic": "TA0043",
                "technique": "AML.T0000",
                "description": "The team first performed reconnaissance to gather information about the target ML model.\n"
            },
            {
                "tactic": "TA0001",
                "technique": "T1078",
                "description": "The team used a valid account to gain access to the network.\n"
            },
            {
                "tactic": "TA0009",
                "technique": "AML.T0035",
                "description": "The team found the model file of the target ML model and the necessary training data.\n"
            },
            {
                "tactic": "AML.TA0001",
                "technique": "AML.T0043.000",
                "description": "Using the target model and data, the red team crafted evasive adversarial data.\n"
            },
            {
                "tactic": "AML.TA0000",
                "technique": "AML.T0040",
                "description": "The team used an exposed API to access the target model.\n"
            },
            {
                "tactic": "TA0040",
                "technique": "AML.T0015",
                "description": "The team performed an online evasion attack by replaying the adversarial examples, which helped achieve this goal.\n"
            }
        ],
        "reported-by": [
            "Microsoft (Azure Trustworthy Machine Learning)"
        ],
        "references": null
    },
    {
        "id": "AML.CS0011",
        "name": "Microsoft Edge AI - Evasion",
        "object-type": "case-study",
        "summary": "The Azure Red Team performed a red team exercise on a new Microsoft product designed for running AI workloads at the Edge.\n",
        "incident-date": "2020-02-01",
        "procedure": [
            {
                "tactic": "TA0043",
                "technique": "AML.T0000",
                "description": "The team first performed reconnaissance to gather information about the target ML model.\n"
            },
            {
                "tactic": "TA0042",
                "technique": "AML.T0002",
                "description": "The team identified and obtained the publically available base model.\n"
            },
            {
                "tactic": "TA0042",
                "technique": "AML.T0002",
                "description": "The team identified and obtained the publically dataset used by the model\n"
            },
            {
                "tactic": "AML.TA0000",
                "technique": "AML.T0040",
                "description": "Then using the publicly available version of the ML model, started sending queries and analyzing the responses (inferences) from the ML model.\n"
            },
            {
                "tactic": "AML.TA0001",
                "technique": "AML.T0043.001",
                "description": "The red team created an automated system that continuously manipulated an original target image, that tricked the ML model into producing incorrect inferences, but the perturbations in the image were unnoticeable to the human eye.\n"
            },
            {
                "tactic": "TA0040",
                "technique": "AML.T0015",
                "description": "Feeding this perturbed image, the red team was able to evade the ML model by causing misclassifications.\n"
            }
        ],
        "reported-by": [
            "Microsoft"
        ],
        "references": null
    },
    {
        "id": "AML.CS0012",
        "name": "MITRE - Physical Adversarial Attack on Face Identification",
        "object-type": "case-study",
        "summary": "MITRE's AI Red Team demonstrated a physical-domain evasion attack on a commercial face identification service with the intention of inducing a targeted misclassification.\nThis operation had a combination of traditional ATT&CK enterprise techniques such as finding Valid account, and Executing code via an API - all interleaved with adversarial ML specific attacks.\n",
        "incident-date": "2020-01-01",
        "procedure": [
            {
                "tactic": "TA0043",
                "technique": "AML.T0000",
                "description": "The team first performed reconnaissance to gather information about the target ML model.\n"
            },
            {
                "tactic": "TA0001",
                "technique": "T1078",
                "description": "The team gained access via a valid account.\n"
            },
            {
                "tactic": "AML.TA0000",
                "technique": "AML.T0040",
                "description": "The team accessed the inference API of the target model.\n"
            },
            {
                "tactic": "TA0007",
                "technique": "AML.T0013",
                "description": "The team identified the list of identities targeted by the model by querying the target model's inference API.\n"
            },
            {
                "tactic": "TA0042",
                "technique": "AML.T0002.000",
                "description": "The team acquired representative open source data.\n"
            },
            {
                "tactic": "TA0042",
                "technique": "AML.T0005",
                "description": "The team developed a proxy model using the open source data.\n"
            },
            {
                "tactic": "AML.TA0001",
                "technique": "AML.T0043.000",
                "description": "Using the proxy model, the red team optimized a physical domain patch-based attack using expectation over transformation.\n"
            },
            {
                "tactic": "AML.TA0000",
                "technique": "AML.T0041",
                "description": "The team placed the physical countermeasure in the physical environment.\n"
            },
            {
                "tactic": "TA0040",
                "technique": "AML.T0015",
                "description": "The team successfully evaded the model using the physical countermeasure and causing targeted misclassifications.\n"
            }
        ],
        "reported-by": [
            "MITRE AI Red Team"
        ],
        "references": null
    }
]